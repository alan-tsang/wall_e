# Custom parameters
defaults:
  - _self_
  - model: simple
  - dataset: simple

defaults:
  - _self_
  - model: simple
  - dataset: simple

# Hydra Configuration for wall_e Framework
# Usage: python train.py data=<choice> model=<choice> metric=<choice>

# ============================================
# Main configuration
run_name: ''
run_dir: './runs'
seed: 42
# this will close wandb、checkpointing and logging
debug: true
training:
  epochs: 10
  # deepspeed config file path
  ds_config: null
  gradient_accumulation: 1
  grad_clip: null
  fp16: false
  print_model: false
  # load previous training state:
  # || seed, model weights, optimizer, scheduler, epoch, iter
  resume_from: null
  load_from: null
  valid_begin_epoch: 2
  valid_interval_epoch: 1
  valid_begin_iter: 10
  valid_interval_iter: 5
  test_begin_epoch: 3
  test_interval_epoch: 1
  test_begin_iter: 60
  test_interval_iter: 5
  progress_every_n_epochs: 1
  progress_every_n_batches: 1
  #  activation_checkpoint: [
  #    'transformer.encoder.layers.0',
  #    'transformer.encoder.layers.1',
  #    'transformer.decoder.layers.0',
  #    'transformer.decoder.layers.1'
  #  ]
optimizer:
  # 默认使用的是AdamW优化器
  lr: 3e-4
  weight_decay: 0.01
scheduler:
  type: "LinearWarmupCosineLRScheduler"
  min_lr: 3e-5
  max_lr: 3e-4
  warmup_rate: 0.05
  warmup_start_lr: 1e-5
log:
  to_file: true
  # options: ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
  level: "INFO"
  rank_level: "WARNING"
checkpoint:
  enable: true
  best_monitor:
    loss: false
  topk: 5
  begin_epoch: 2
  epoch_interval: 2
  begin_batch: 10
  batch_interval: 10
wandb:
  enable: true
  proj_name: "wall_e example: copy str"
  offline: true
  tags: ["wall_e", "example"]
