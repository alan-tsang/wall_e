# Custom parameters
defaults:
  - _self_
  - model: simple
  - dataset: simple

# Hydra Configuration for wall_e Framework
# Usage: python train.py data=<choice> model=<choice> metric=<choice>

# ============================================
# Main configuration
run_name: ''
run_dir: './runs'
seed: 42
# this will close wandb、checkpointing and logging
debug: true
training:
  epochs: 10
  # deepspeed config file path
  ds_config: null
  fsdp:
    enable: false
    # options: sharded | full
    # - sharded: faster + smaller, good for resume training
    # - full: single-file checkpoint on rank0, good for export
    state_dict_type: sharded
    # options: bf16 | fp16 | null
    mixed_precision: bf16
    # options: transformer | null
    # transformer: shard at the specified block/layer level
    # null: shard at each parameter level
    auto_wrap_policy: null
    # Which module(s) to wrap when auto_wrap_policy=transformer.
    # You can specify either:
    # 1) wrap_modules: ["transformer.encoder.layers.0", "transformer.encoder.layers.1", ...]  (model path, recommended)
    # 2) wrap_blocks:  ["GPT2Block", "LlamaDecoderLayer", ...]                                 (class name)
    # Using wrap_modules mirrors training.activation_checkpoint style and is the most direct + convenient.
    wrap_modules: null
    # Which module class(es) to wrap when auto_wrap_policy=transformer (fallback).
    wrap_blocks: null
  gradient_accumulation: 1
  grad_clip: null
  fp16: false
  print_model: false
  # load previous training state:
  # || seed, model weights, optimizer, scheduler, epoch, iter
  resume_from: null
  load_from: null
  valid_begin_epoch: 2
  valid_interval_epoch: 1
  valid_begin_iter: 10
  valid_interval_iter: 5
  test_begin_epoch: 3
  test_interval_epoch: 1
  test_begin_iter: 60
  test_interval_iter: 5
  progress_every_n_epochs: 1
  progress_every_n_batches: 1
  #  activation_checkpoint: [
  #    'transformer.encoder.layers.0',
  #    'transformer.encoder.layers.1',
  #    'transformer.decoder.layers.0',
  #    'transformer.decoder.layers.1'
  #  ]
optimizer:
  # 默认使用的是AdamW优化器
  lr: 3e-4
  weight_decay: 0.01
scheduler:
  type: "LinearWarmupCosineLRScheduler"
  min_lr: 3e-5
  max_lr: 3e-4
  warmup_rate: 0.05
  warmup_start_lr: 1e-5
log:
  to_file: true
  # options: ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
  level: "INFO"
  rank_level: "WARNING"
checkpoint:
  enable: true
  best_monitor:
    loss: false
  topk: 5
  begin_epoch: 2
  epoch_interval: 2
  begin_batch: 10
  batch_interval: 10
wandb:
  enable: true
  proj_name: "wall_e example: copy str"
  offline: true
  tags: ["wall_e", "example"]
